{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c9e3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd9786a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "494          495         0       3   \n",
      "495          496         0       3   \n",
      "496          497         1       1   \n",
      "497          498         0       3   \n",
      "498          499         0       1   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "494                         Stanley, Mr. Edward Roland    male  21.0      0   \n",
      "495                              Yousseff, Mr. Gerious    male   NaN      0   \n",
      "496                     Eustis, Miss. Elizabeth Mussey  female  54.0      1   \n",
      "497                    Shellard, Mr. Frederick William    male   NaN      0   \n",
      "498    Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female  25.0      1   \n",
      "\n",
      "     Parch            Ticket      Fare    Cabin Embarked  \n",
      "0        0         A/5 21171    7.2500      NaN        S  \n",
      "1        0          PC 17599   71.2833      C85        C  \n",
      "2        0  STON/O2. 3101282    7.9250      NaN        S  \n",
      "3        0            113803   53.1000     C123        S  \n",
      "4        0            373450    8.0500      NaN        S  \n",
      "..     ...               ...       ...      ...      ...  \n",
      "494      0         A/4 45380    8.0500      NaN        S  \n",
      "495      0              2627   14.4583      NaN        C  \n",
      "496      0             36947   78.2667      D20        C  \n",
      "497      0         C.A. 6212   15.1000      NaN        S  \n",
      "498      2            113781  151.5500  C22 C26        S  \n",
      "\n",
      "[499 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9ed6e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass     Sex   Age  SibSp  Parch      Fare\n",
      "0         3    male  22.0      1      0    7.2500\n",
      "1         1  female  38.0      1      0   71.2833\n",
      "2         3  female  26.0      0      0    7.9250\n",
      "3         1  female  35.0      1      0   53.1000\n",
      "4         3    male  35.0      0      0    8.0500\n",
      "..      ...     ...   ...    ...    ...       ...\n",
      "494       3    male  21.0      0      0    8.0500\n",
      "495       3    male   NaN      0      0   14.4583\n",
      "496       1  female  54.0      1      0   78.2667\n",
      "497       3    male   NaN      0      0   15.1000\n",
      "498       1  female  25.0      1      2  151.5500\n",
      "\n",
      "[499 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "y = df['Survived'].values.reshape(-1,1)\n",
    "data = df.drop([\"Name\",'Ticket','Cabin','Embarked','PassengerId','Survived'], axis=1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "795593dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass      0\n",
      "Sex         0\n",
      "Age       102\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32478364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass     Sex        Age  SibSp  Parch      Fare\n",
      "0         3    male  22.000000      1      0    7.2500\n",
      "1         1  female  38.000000      1      0   71.2833\n",
      "2         3  female  26.000000      0      0    7.9250\n",
      "3         1  female  35.000000      1      0   53.1000\n",
      "4         3    male  35.000000      0      0    8.0500\n",
      "..      ...     ...        ...    ...    ...       ...\n",
      "494       3    male  21.000000      0      0    8.0500\n",
      "495       3    male  29.217884      0      0   14.4583\n",
      "496       1  female  54.000000      1      0   78.2667\n",
      "497       3    male  29.217884      0      0   15.1000\n",
      "498       1  female  25.000000      1      2  151.5500\n",
      "\n",
      "[499 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "data['Age'] = data['Age'].fillna(data['Age'].mean())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca1919ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass  Sex        Age  SibSp  Parch      Fare\n",
      "0         3    1  22.000000      1      0    7.2500\n",
      "1         1    2  38.000000      1      0   71.2833\n",
      "2         3    2  26.000000      0      0    7.9250\n",
      "3         1    2  35.000000      1      0   53.1000\n",
      "4         3    1  35.000000      0      0    8.0500\n",
      "..      ...  ...        ...    ...    ...       ...\n",
      "494       3    1  21.000000      0      0    8.0500\n",
      "495       3    1  29.217884      0      0   14.4583\n",
      "496       1    2  54.000000      1      0   78.2667\n",
      "497       3    1  29.217884      0      0   15.1000\n",
      "498       1    2  25.000000      1      2  151.5500\n",
      "\n",
      "[499 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sex'] = data['Sex'].map({'male': 1 ,'female':2})\n",
    "\n",
    "print(data)\n",
    "m = data.shape[0]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fce6ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for iteration  1 is 0.6948930344303106\n",
      "loss for iteration  2 is 0.681011845705105\n",
      "loss for iteration  3 is 0.6741820670565937\n",
      "loss for iteration  4 is 0.6707701747579323\n",
      "loss for iteration  5 is 0.6690231009884579\n",
      "loss for iteration  6 is 0.668093074463766\n",
      "loss for iteration  7 is 0.667566835589925\n",
      "loss for iteration  8 is 0.6672411588375174\n",
      "loss for iteration  9 is 0.6670153063861453\n",
      "loss for iteration  10 is 0.6668389939728293\n",
      "loss for iteration  11 is 0.6666869582469632\n",
      "loss for iteration  12 is 0.6665464159207318\n",
      "loss for iteration  13 is 0.6664108357029901\n",
      "loss for iteration  14 is 0.6662768281572172\n",
      "loss for iteration  15 is 0.6661425860091181\n",
      "loss for iteration  16 is 0.6660070994673459\n",
      "loss for iteration  17 is 0.6658697604684785\n",
      "loss for iteration  18 is 0.6657301626701086\n",
      "loss for iteration  19 is 0.6655880001821753\n",
      "loss for iteration  20 is 0.6654430161858295\n",
      "loss for iteration  21 is 0.6652949767898595\n",
      "loss for iteration  22 is 0.6651436576671945\n",
      "loss for iteration  23 is 0.6649888371684742\n",
      "loss for iteration  24 is 0.6648302927207843\n",
      "loss for iteration  25 is 0.664667798894086\n",
      "loss for iteration  26 is 0.6645011263153167\n",
      "loss for iteration  27 is 0.6643300410143342\n",
      "loss for iteration  28 is 0.6641543039908744\n",
      "loss for iteration  29 is 0.6639736708957167\n",
      "loss for iteration  30 is 0.6637878917720907\n",
      "loss for iteration  31 is 0.6635967108301984\n",
      "loss for iteration  32 is 0.6633998662414042\n",
      "loss for iteration  33 is 0.6631970899456195\n",
      "loss for iteration  34 is 0.6629881074689908\n",
      "loss for iteration  35 is 0.6627726377508497\n",
      "loss for iteration  36 is 0.6625503929798701\n",
      "loss for iteration  37 is 0.6623210784399044\n",
      "loss for iteration  38 is 0.6620843923662881\n",
      "loss for iteration  39 is 0.6618400258135905\n",
      "loss for iteration  40 is 0.6615876625359327\n",
      "loss for iteration  41 is 0.6613269788811059\n",
      "loss for iteration  42 is 0.6610576436998106\n",
      "loss for iteration  43 is 0.6607793182714367\n",
      "loss for iteration  44 is 0.6604916562478798\n",
      "loss for iteration  45 is 0.6601943036169784\n",
      "loss for iteration  46 is 0.6598868986872315\n",
      "loss for iteration  47 is 0.6595690720955384\n",
      "loss for iteration  48 is 0.6592404468397737\n",
      "loss for iteration  49 is 0.658900638338086\n",
      "loss for iteration  50 is 0.6585492545168731\n",
      "loss for iteration  51 is 0.6581858959294477\n",
      "loss for iteration  52 is 0.6578101559074657\n",
      "loss for iteration  53 is 0.6574216207472346\n",
      "loss for iteration  54 is 0.6570198699330541\n",
      "loss for iteration  55 is 0.6566044763997765\n",
      "loss for iteration  56 is 0.656175006836779\n",
      "loss for iteration  57 is 0.6557310220355517\n",
      "loss for iteration  58 is 0.6552720772830839\n",
      "loss for iteration  59 is 0.6547977228032031\n",
      "loss for iteration  60 is 0.6543075042479721\n",
      "loss for iteration  61 is 0.6538009632411755\n",
      "loss for iteration  62 is 0.6532776379758412\n",
      "loss for iteration  63 is 0.6527370638676154\n",
      "loss for iteration  64 is 0.6521787742656782\n",
      "loss for iteration  65 is 0.651602301222708\n",
      "loss for iteration  66 is 0.651007176325208\n",
      "loss for iteration  67 is 0.6503929315852816\n",
      "loss for iteration  68 is 0.6497591003946742\n",
      "loss for iteration  69 is 0.6491052185416192\n",
      "loss for iteration  70 is 0.6484308252906856\n",
      "loss for iteration  71 is 0.6477354645254793\n",
      "loss for iteration  72 is 0.6470186859536451\n",
      "loss for iteration  73 is 0.6462800463732016\n",
      "loss for iteration  74 is 0.6455191109987752\n",
      "loss for iteration  75 is 0.6447354548458158\n",
      "loss for iteration  76 is 0.6439286641703561\n",
      "loss for iteration  77 is 0.643098337961334\n",
      "loss for iteration  78 is 0.642244089481929\n",
      "loss for iteration  79 is 0.6413655478557733\n",
      "loss for iteration  80 is 0.6404623596932975\n",
      "loss for iteration  81 is 0.6395341907528523\n",
      "loss for iteration  82 is 0.6385807276306277\n",
      "loss for iteration  83 is 0.63760167947277\n",
      "loss for iteration  84 is 0.6365967797024843\n",
      "loss for iteration  85 is 0.6355657877543066\n",
      "loss for iteration  86 is 0.6345084908071518\n",
      "loss for iteration  87 is 0.6334247055072003\n",
      "loss for iteration  88 is 0.6323142796711638\n",
      "loss for iteration  89 is 0.6311770939600134\n",
      "loss for iteration  90 is 0.6300130635128411\n",
      "loss for iteration  91 is 0.6288221395301757\n",
      "loss for iteration  92 is 0.6276043107957977\n",
      "loss for iteration  93 is 0.6263596051259085\n",
      "loss for iteration  94 is 0.6250880907343883\n",
      "loss for iteration  95 is 0.6237898775028737\n",
      "loss for iteration  96 is 0.6224651181444613\n",
      "loss for iteration  97 is 0.6211140092500408\n",
      "loss for iteration  98 is 0.6197367922065588\n",
      "loss for iteration  99 is 0.6183337539769218\n",
      "loss for iteration  100 is 0.6169052277317817\n",
      "loss for iteration  101 is 0.6154515933240786\n",
      "loss for iteration  102 is 0.6139732775979709\n",
      "loss for iteration  103 is 0.6124707545246418\n",
      "loss for iteration  104 is 0.6109445451584389\n",
      "loss for iteration  105 is 0.6093952174078617\n",
      "loss for iteration  106 is 0.6078233856170674\n",
      "loss for iteration  107 is 0.6062297099547945\n",
      "loss for iteration  108 is 0.6046148956089078\n",
      "loss for iteration  109 is 0.6029796917861203\n",
      "loss for iteration  110 is 0.6013248905178515\n",
      "loss for iteration  111 is 0.5996513252746051\n",
      "loss for iteration  112 is 0.5979598693926912\n",
      "loss for iteration  113 is 0.5962514343185521\n",
      "loss for iteration  114 is 0.5945269676773642\n",
      "loss for iteration  115 is 0.5927874511739749\n",
      "loss for iteration  116 is 0.5910338983355474\n",
      "loss for iteration  117 is 0.5892673521065566\n",
      "loss for iteration  118 is 0.5874888823079428\n",
      "loss for iteration  119 is 0.5856995829733138\n",
      "loss for iteration  120 is 0.5839005695760439\n",
      "loss for iteration  121 is 0.582092976161974\n",
      "loss for iteration  122 is 0.5802779524031152\n",
      "loss for iteration  123 is 0.5784566605883454\n",
      "loss for iteration  124 is 0.5766302725675057\n",
      "loss for iteration  125 is 0.5747999666655879\n",
      "loss for iteration  126 is 0.5729669245838271\n",
      "loss for iteration  127 is 0.5711323283044905\n",
      "loss for iteration  128 is 0.5692973570159751\n",
      "loss for iteration  129 is 0.5674631840745105\n",
      "loss for iteration  130 is 0.5656309740182949\n",
      "loss for iteration  131 is 0.5638018796493063\n",
      "loss for iteration  132 is 0.5619770391973069\n",
      "loss for iteration  133 is 0.5601575735797287\n",
      "loss for iteration  134 is 0.5583445837702006\n",
      "loss for iteration  135 is 0.5565391482874457\n",
      "loss for iteration  136 is 0.5547423208151915\n",
      "loss for iteration  137 is 0.552955127962567\n",
      "loss for iteration  138 is 0.5511785671732566\n",
      "loss for iteration  139 is 0.5494136047904442\n",
      "loss for iteration  140 is 0.5476611742833112\n",
      "loss for iteration  141 is 0.5459221746395978\n",
      "loss for iteration  142 is 0.5441974689274728\n",
      "loss for iteration  143 is 0.5424878830287188\n",
      "loss for iteration  144 is 0.5407942045440411\n",
      "loss for iteration  145 is 0.5391171818701403\n",
      "loss for iteration  146 is 0.5374575234470815\n",
      "loss for iteration  147 is 0.5358158971734502\n",
      "loss for iteration  148 is 0.5341929299857953\n",
      "loss for iteration  149 is 0.532589207597968\n",
      "loss for iteration  150 is 0.5310052743951339\n",
      "loss for iteration  151 is 0.5294416334764982\n",
      "loss for iteration  152 is 0.5278987468401299\n",
      "loss for iteration  153 is 0.5263770357027067\n",
      "loss for iteration  154 is 0.5248768809465197\n",
      "loss for iteration  155 is 0.5233986236856919\n",
      "loss for iteration  156 is 0.5219425659432537\n",
      "loss for iteration  157 is 0.5205089714304983\n",
      "loss for iteration  158 is 0.5190980664198958\n",
      "loss for iteration  159 is 0.5177100407027757\n",
      "loss for iteration  160 is 0.5163450486229944\n",
      "loss for iteration  161 is 0.5150032101778648\n",
      "loss for iteration  162 is 0.5136846121777634\n",
      "loss for iteration  163 is 0.5123893094560099\n",
      "loss for iteration  164 is 0.5111173261208455\n",
      "loss for iteration  165 is 0.5098686568416186\n",
      "loss for iteration  166 is 0.5086432681615967\n",
      "loss for iteration  167 is 0.50744109983017\n",
      "loss for iteration  168 is 0.506262066147587\n",
      "loss for iteration  169 is 0.5051060573157524\n",
      "loss for iteration  170 is 0.5039729407890304\n",
      "loss for iteration  171 is 0.5028625626194141\n",
      "loss for iteration  172 is 0.5017747487908497\n",
      "loss for iteration  173 is 0.5007093065379321\n",
      "loss for iteration  174 is 0.49966602564461954\n",
      "loss for iteration  175 is 0.49864467971903087\n",
      "loss for iteration  176 is 0.4976450274408111\n",
      "loss for iteration  177 is 0.49666681377795135\n",
      "loss for iteration  178 is 0.49570977117033854\n",
      "loss for iteration  179 is 0.4947736206776915\n",
      "loss for iteration  180 is 0.4938580730898952\n",
      "loss for iteration  181 is 0.49296282999808905\n",
      "loss for iteration  182 is 0.4920875848251869\n",
      "loss for iteration  183 is 0.4912320238148111\n",
      "loss for iteration  184 is 0.4903958269779053\n",
      "loss for iteration  185 is 0.4895786689965526\n",
      "loss for iteration  186 is 0.48878022008477223\n",
      "loss for iteration  187 is 0.488000146806285\n",
      "loss for iteration  188 is 0.48723811284944535\n",
      "loss for iteration  189 is 0.48649377975971775\n",
      "loss for iteration  190 is 0.4857668076302424\n",
      "loss for iteration  191 is 0.48505685575117957\n",
      "loss for iteration  192 is 0.48436358321865153\n",
      "loss for iteration  193 is 0.48368664950421436\n",
      "loss for iteration  194 is 0.48302571498588914\n",
      "loss for iteration  195 is 0.48238044144186204\n",
      "loss for iteration  196 is 0.481750492508035\n",
      "loss for iteration  197 is 0.48113553410066096\n",
      "loss for iteration  198 is 0.4805352348053431\n",
      "loss for iteration  199 is 0.47994926623371204\n",
      "loss for iteration  200 is 0.4793773033491123\n",
      "loss for iteration  201 is 0.47881902476265226\n",
      "loss for iteration  202 is 0.47827411300096523\n",
      "loss for iteration  203 is 0.47774225474704063\n",
      "loss for iteration  204 is 0.4772231410554633\n",
      "loss for iteration  205 is 0.47671646754339325\n",
      "loss for iteration  206 is 0.4762219345585932\n",
      "loss for iteration  207 is 0.4757392473257907\n",
      "loss for iteration  208 is 0.4752681160726289\n",
      "loss for iteration  209 is 0.47480825613643307\n",
      "loss for iteration  210 is 0.4743593880529828\n",
      "loss for iteration  211 is 0.4739212376284437\n",
      "loss for iteration  212 is 0.4734935359955746\n",
      "loss for iteration  213 is 0.47307601965528456\n",
      "loss for iteration  214 is 0.472668430504576\n",
      "loss for iteration  215 is 0.4722705158518652\n",
      "loss for iteration  216 is 0.4718820284206312\n",
      "loss for iteration  217 is 0.47150272634230134\n",
      "loss for iteration  218 is 0.47113237313923995\n",
      "loss for iteration  219 is 0.47077073769866407\n",
      "loss for iteration  220 is 0.47041759423826834\n",
      "loss for iteration  221 is 0.47007272226430297\n",
      "loss for iteration  222 is 0.46973590652280545\n",
      "loss for iteration  223 is 0.46940693694464986\n",
      "loss for iteration  224 is 0.4690856085850392\n",
      "loss for iteration  225 is 0.4687717215580297\n",
      "loss for iteration  226 is 0.4684650809666386\n",
      "loss for iteration  227 is 0.46816549682905506\n",
      "loss for iteration  228 is 0.46787278400144006\n",
      "loss for iteration  229 is 0.46758676209776545\n",
      "loss for iteration  230 is 0.4673072554071195\n",
      "loss for iteration  231 is 0.46703409280886843\n",
      "loss for iteration  232 is 0.46676710768604224\n",
      "loss for iteration  233 is 0.46650613783728206\n",
      "loss for iteration  234 is 0.46625102538766294\n",
      "loss for iteration  235 is 0.4660016166986826\n",
      "loss for iteration  236 is 0.4657577622776811\n",
      "loss for iteration  237 is 0.46551931668693763\n",
      "loss for iteration  238 is 0.46528613845266814\n",
      "loss for iteration  239 is 0.4650580899741299\n",
      "loss for iteration  240 is 0.4648350374330192\n",
      "loss for iteration  241 is 0.46461685070333253\n",
      "loss for iteration  242 is 0.4644034032618455\n",
      "loss for iteration  243 is 0.4641945720993473\n",
      "loss for iteration  244 is 0.46399023763275665\n",
      "loss for iteration  245 is 0.4637902836182303\n",
      "loss for iteration  246 is 0.46359459706536277\n",
      "loss for iteration  247 is 0.4634030681525657\n",
      "loss for iteration  248 is 0.4632155901437043\n",
      "loss for iteration  249 is 0.4630320593060572\n",
      "loss for iteration  250 is 0.46285237482965785\n",
      "loss for iteration  251 is 0.46267643874806796\n",
      "loss for iteration  252 is 0.46250415586062277\n",
      "loss for iteration  253 is 0.4623354336561844\n",
      "loss for iteration  254 is 0.4621701822384299\n",
      "loss for iteration  255 is 0.46200831425269534\n",
      "loss for iteration  256 is 0.4618497448143927\n",
      "loss for iteration  257 is 0.4616943914390097\n",
      "loss for iteration  258 is 0.4615421739736986\n",
      "loss for iteration  259 is 0.4613930145304563\n",
      "loss for iteration  260 is 0.4612468374208928\n",
      "loss for iteration  261 is 0.4611035690925835\n",
      "loss for iteration  262 is 0.460963138066995\n",
      "loss for iteration  263 is 0.4608254748789744\n",
      "loss for iteration  264 is 0.4606905120177873\n",
      "loss for iteration  265 is 0.4605581838696866\n",
      "loss for iteration  266 is 0.46042842666199685\n",
      "loss for iteration  267 is 0.4603011784086901\n",
      "loss for iteration  268 is 0.46017637885743445\n",
      "loss for iteration  269 is 0.46005396943809046\n",
      "loss for iteration  270 is 0.45993389321263156\n",
      "loss for iteration  271 is 0.4598160948264627\n",
      "loss for iteration  272 is 0.45970052046111254\n",
      "loss for iteration  273 is 0.4595871177882698\n",
      "loss for iteration  274 is 0.4594758359251388\n",
      "loss for iteration  275 is 0.4593666253910847\n",
      "loss for iteration  276 is 0.4592594380655409\n",
      "loss for iteration  277 is 0.4591542271471503\n",
      "loss for iteration  278 is 0.4590509471141117\n",
      "loss for iteration  279 is 0.45894955368570245\n",
      "loss for iteration  280 is 0.4588500037849492\n",
      "loss for iteration  281 is 0.4587522555024186\n",
      "loss for iteration  282 is 0.45865626806109855\n",
      "loss for iteration  283 is 0.4585620017823434\n",
      "loss for iteration  284 is 0.4584694180528537\n",
      "loss for iteration  285 is 0.4583784792926643\n",
      "loss for iteration  286 is 0.45828914892411304\n",
      "loss for iteration  287 is 0.4582013913417631\n",
      "loss for iteration  288 is 0.4581151718832529\n",
      "loss for iteration  289 is 0.4580304568010475\n",
      "loss for iteration  290 is 0.4579472132350657\n",
      "loss for iteration  291 is 0.4578654091861589\n",
      "loss for iteration  292 is 0.4577850134904147\n",
      "loss for iteration  293 is 0.4577059957942646\n",
      "loss for iteration  294 is 0.4576283265303691\n",
      "loss for iteration  295 is 0.4575519768942585\n",
      "loss for iteration  296 is 0.4574769188217081\n",
      "loss for iteration  297 is 0.4574031249668225\n",
      "loss for iteration  298 is 0.45733056868081123\n",
      "loss for iteration  299 is 0.4572592239914317\n",
      "loss for iteration  300 is 0.45718906558308103\n",
      "loss for iteration  301 is 0.45712006877751615\n",
      "loss for iteration  302 is 0.45705220951518194\n",
      "loss for iteration  303 is 0.45698546433713\n",
      "loss for iteration  304 is 0.45691981036750867\n",
      "loss for iteration  305 is 0.4568552252966065\n",
      "loss for iteration  306 is 0.4567916873644319\n",
      "loss for iteration  307 is 0.45672917534481217\n",
      "loss for iteration  308 is 0.45666766852999513\n",
      "loss for iteration  309 is 0.4566071467157378\n",
      "loss for iteration  310 is 0.45654759018686597\n",
      "loss for iteration  311 is 0.45648897970329033\n",
      "loss for iteration  312 is 0.4564312964864642\n",
      "loss for iteration  313 is 0.4563745222062681\n",
      "loss for iteration  314 is 0.4563186389683089\n",
      "loss for iteration  315 is 0.45626362930161835\n",
      "loss for iteration  316 is 0.45620947614673935\n",
      "loss for iteration  317 is 0.45615616284418714\n",
      "loss for iteration  318 is 0.45610367312327255\n",
      "loss for iteration  319 is 0.456051991091277\n",
      "loss for iteration  320 is 0.4560011012229654\n",
      "loss for iteration  321 is 0.4559509883504292\n",
      "loss for iteration  322 is 0.4559016376532455\n",
      "loss for iteration  323 is 0.4558530346489443\n",
      "loss for iteration  324 is 0.4558051651837724\n",
      "loss for iteration  325 is 0.4557580154237451\n",
      "loss for iteration  326 is 0.45571157184597555\n",
      "loss for iteration  327 is 0.4556658212302735\n",
      "loss for iteration  328 is 0.4556207506510034\n",
      "loss for iteration  329 is 0.45557634746919473\n",
      "loss for iteration  330 is 0.4555325993248951\n",
      "loss for iteration  331 is 0.45548949412975875\n",
      "loss for iteration  332 is 0.4554470200598628\n",
      "loss for iteration  333 is 0.45540516554874316\n",
      "loss for iteration  334 is 0.4553639192806435\n",
      "loss for iteration  335 is 0.45532327018396984\n",
      "loss for iteration  336 is 0.4552832074249445\n",
      "loss for iteration  337 is 0.4552437204014522\n",
      "loss for iteration  338 is 0.4552047987370725\n",
      "loss for iteration  339 is 0.45516643227529185\n",
      "loss for iteration  340 is 0.45512861107389074\n",
      "loss for iteration  341 is 0.4550913253994986\n",
      "loss for iteration  342 is 0.45505456572231096\n",
      "loss for iteration  343 is 0.45501832271096543\n",
      "loss for iteration  344 is 0.45498258722756946\n",
      "loss for iteration  345 is 0.4549473503228749\n",
      "loss for iteration  346 is 0.4549126032315963\n",
      "loss for iteration  347 is 0.45487833736786615\n",
      "loss for iteration  348 is 0.4548445443208234\n",
      "loss for iteration  349 is 0.45481121585033224\n",
      "loss for iteration  350 is 0.4547783438828241\n",
      "loss for iteration  351 is 0.45474592050726165\n",
      "loss for iteration  352 is 0.45471393797121884\n",
      "loss for iteration  353 is 0.4546823886770741\n",
      "loss for iteration  354 is 0.45465126517831367\n",
      "loss for iteration  355 is 0.4546205601759394\n",
      "loss for iteration  356 is 0.4545902665149799\n",
      "loss for iteration  357 is 0.45456037718110065\n",
      "loss for iteration  358 is 0.45453088529730984\n",
      "loss for iteration  359 is 0.4545017841207566\n",
      "loss for iteration  360 is 0.4544730670396201\n",
      "loss for iteration  361 is 0.4544447275700845\n",
      "loss for iteration  362 is 0.4544167593533985\n",
      "loss for iteration  363 is 0.4543891561530165\n",
      "loss for iteration  364 is 0.4543619118518184\n",
      "loss for iteration  365 is 0.45433502044940594\n",
      "loss for iteration  366 is 0.45430847605947233\n",
      "loss for iteration  367 is 0.45428227290724454\n",
      "loss for iteration  368 is 0.454256405326994\n",
      "loss for iteration  369 is 0.4542308677596149\n",
      "loss for iteration  370 is 0.4542056547502673\n",
      "loss for iteration  371 is 0.4541807609460834\n",
      "loss for iteration  372 is 0.45415618109393463\n",
      "loss for iteration  373 is 0.45413191003825776\n",
      "loss for iteration  374 is 0.45410794271893895\n",
      "loss for iteration  375 is 0.4540842741692525\n",
      "loss for iteration  376 is 0.4540608995138536\n",
      "loss for iteration  377 is 0.4540378139668234\n",
      "loss for iteration  378 is 0.4540150128297648\n",
      "loss for iteration  379 is 0.45399249148994597\n",
      "loss for iteration  380 is 0.4539702454184936\n",
      "loss for iteration  381 is 0.45394827016862954\n",
      "loss for iteration  382 is 0.4539265613739542\n",
      "loss for iteration  383 is 0.45390511474677214\n",
      "loss for iteration  384 is 0.4538839260764599\n",
      "loss for iteration  385 is 0.4538629912278745\n",
      "loss for iteration  386 is 0.4538423061398016\n",
      "loss for iteration  387 is 0.453821866823442\n",
      "loss for iteration  388 is 0.4538016693609346\n",
      "loss for iteration  389 is 0.4537817099039169\n",
      "loss for iteration  390 is 0.4537619846721192\n",
      "loss for iteration  391 is 0.453742489951993\n",
      "loss for iteration  392 is 0.45372322209537325\n",
      "loss for iteration  393 is 0.4537041775181717\n",
      "loss for iteration  394 is 0.4536853526991018\n",
      "loss for iteration  395 is 0.4536667441784338\n",
      "loss for iteration  396 is 0.45364834855677905\n",
      "loss for iteration  397 is 0.453630162493903\n",
      "loss for iteration  398 is 0.45361218270756576\n",
      "loss for iteration  399 is 0.4535944059723894\n",
      "loss for iteration  400 is 0.45357682911875224\n",
      "loss for iteration  401 is 0.4535594490317072\n",
      "loss for iteration  402 is 0.45354226264992603\n",
      "loss for iteration  403 is 0.4535252669646671\n",
      "loss for iteration  404 is 0.4535084590187665\n",
      "loss for iteration  405 is 0.45349183590565195\n",
      "loss for iteration  406 is 0.45347539476837884\n",
      "loss for iteration  407 is 0.4534591327986877\n",
      "loss for iteration  408 is 0.4534430472360827\n",
      "loss for iteration  409 is 0.45342713536693\n",
      "loss for iteration  410 is 0.4534113945235768\n",
      "loss for iteration  411 is 0.4533958220834888\n",
      "loss for iteration  412 is 0.45338041546840685\n",
      "loss for iteration  413 is 0.45336517214352173\n",
      "loss for iteration  414 is 0.453350089616667\n",
      "loss for iteration  415 is 0.4533351654375285\n",
      "loss for iteration  416 is 0.4533203971968714\n",
      "loss for iteration  417 is 0.4533057825257832\n",
      "loss for iteration  418 is 0.4532913190949331\n",
      "loss for iteration  419 is 0.4532770046138469\n",
      "loss for iteration  420 is 0.4532628368301968\n",
      "loss for iteration  421 is 0.453248813529106\n",
      "loss for iteration  422 is 0.4532349325324686\n",
      "loss for iteration  423 is 0.4532211916982823\n",
      "loss for iteration  424 is 0.45320758891999563\n",
      "loss for iteration  425 is 0.45319412212586846\n",
      "loss for iteration  426 is 0.45318078927834565\n",
      "loss for iteration  427 is 0.4531675883734428\n",
      "loss for iteration  428 is 0.4531545174401454\n",
      "loss for iteration  429 is 0.4531415745398192\n",
      "loss for iteration  430 is 0.45312875776563316\n",
      "loss for iteration  431 is 0.45311606524199344\n",
      "loss for iteration  432 is 0.45310349512398884\n",
      "loss for iteration  433 is 0.4530910455968472\n",
      "loss for iteration  434 is 0.45307871487540236\n",
      "loss for iteration  435 is 0.4530665012035726\n",
      "loss for iteration  436 is 0.4530544028538476\n",
      "loss for iteration  437 is 0.4530424181267871\n",
      "loss for iteration  438 is 0.45303054535052795\n",
      "loss for iteration  439 is 0.4530187828803022\n",
      "loss for iteration  440 is 0.45300712909796276\n",
      "loss for iteration  441 is 0.45299558241152005\n",
      "loss for iteration  442 is 0.45298414125468595\n",
      "loss for iteration  443 is 0.4529728040864274\n",
      "loss for iteration  444 is 0.4529615693905285\n",
      "loss for iteration  445 is 0.45295043567516025\n",
      "loss for iteration  446 is 0.4529394014724594\n",
      "loss for iteration  447 is 0.4529284653381146\n",
      "loss for iteration  448 is 0.4529176258509603\n",
      "loss for iteration  449 is 0.452906881612579\n",
      "loss for iteration  450 is 0.45289623124691003\n",
      "loss for iteration  451 is 0.45288567339986646\n",
      "loss for iteration  452 is 0.4528752067389582\n",
      "loss for iteration  453 is 0.4528648299529233\n",
      "loss for iteration  454 is 0.4528545417513651\n",
      "loss for iteration  455 is 0.4528443408643963\n",
      "loss for iteration  456 is 0.45283422604229\n",
      "loss for iteration  457 is 0.45282419605513674\n",
      "loss for iteration  458 is 0.45281424969250805\n",
      "loss for iteration  459 is 0.45280438576312565\n",
      "loss for iteration  460 is 0.4527946030945378\n",
      "loss for iteration  461 is 0.4527849005328002\n",
      "loss for iteration  462 is 0.45277527694216335\n",
      "loss for iteration  463 is 0.4527657312047661\n",
      "loss for iteration  464 is 0.4527562622203335\n",
      "loss for iteration  465 is 0.45274686890588134\n",
      "loss for iteration  466 is 0.45273755019542483\n",
      "loss for iteration  467 is 0.45272830503969386\n",
      "loss for iteration  468 is 0.4527191324058517\n",
      "loss for iteration  469 is 0.45271003127722087\n",
      "loss for iteration  470 is 0.4527010006530113\n",
      "loss for iteration  471 is 0.45269203954805587\n",
      "loss for iteration  472 is 0.45268314699254897\n",
      "loss for iteration  473 is 0.4526743220317902\n",
      "loss for iteration  474 is 0.4526655637259331\n",
      "loss for iteration  475 is 0.45265687114973696\n",
      "loss for iteration  476 is 0.4526482433923251\n",
      "loss for iteration  477 is 0.4526396795569455\n",
      "loss for iteration  478 is 0.4526311787607362\n",
      "loss for iteration  479 is 0.4526227401344959\n",
      "loss for iteration  480 is 0.4526143628224566\n",
      "loss for iteration  481 is 0.4526060459820621\n",
      "loss for iteration  482 is 0.4525977887837491\n",
      "loss for iteration  483 is 0.4525895904107331\n",
      "loss for iteration  484 is 0.45258145005879696\n",
      "loss for iteration  485 is 0.45257336693608424\n",
      "loss for iteration  486 is 0.45256534026289524\n",
      "loss for iteration  487 is 0.4525573692714869\n",
      "loss for iteration  488 is 0.45254945320587686\n",
      "loss for iteration  489 is 0.45254159132164945\n",
      "loss for iteration  490 is 0.4525337828857666\n",
      "loss for iteration  491 is 0.4525260271763807\n",
      "loss for iteration  492 is 0.45251832348265153\n",
      "loss for iteration  493 is 0.4525106711045661\n",
      "loss for iteration  494 is 0.4525030693527617\n",
      "loss for iteration  495 is 0.45249551754835143\n",
      "loss for iteration  496 is 0.45248801502275393\n",
      "loss for iteration  497 is 0.4524805611175244\n",
      "loss for iteration  498 is 0.45247315518419007\n",
      "loss for iteration  499 is 0.4524657965840874\n",
      "loss for iteration  500 is 0.45245848468820293\n",
      "loss for iteration  501 is 0.4524512188770156\n",
      "loss for iteration  502 is 0.4524439985403435\n",
      "loss for iteration  503 is 0.4524368230771918\n",
      "loss for iteration  504 is 0.45242969189560356\n",
      "loss for iteration  505 is 0.45242260441251403\n",
      "loss for iteration  506 is 0.45241556005360556\n",
      "loss for iteration  507 is 0.4524085582531674\n",
      "loss for iteration  508 is 0.45240159845395544\n",
      "loss for iteration  509 is 0.4523946801070563\n",
      "loss for iteration  510 is 0.4523878026717524\n",
      "loss for iteration  511 is 0.45238096561539\n",
      "loss for iteration  512 is 0.4523741684132493\n",
      "loss for iteration  513 is 0.4523674105484166\n",
      "loss for iteration  514 is 0.4523606915116592\n",
      "loss for iteration  515 is 0.4523540108013011\n",
      "loss for iteration  516 is 0.45234736792310254\n",
      "loss for iteration  517 is 0.45234076239014015\n",
      "loss for iteration  518 is 0.4523341937226899\n",
      "loss for iteration  519 is 0.45232766144811165\n",
      "loss for iteration  520 is 0.45232116510073583\n",
      "loss for iteration  521 is 0.45231470422175213\n",
      "loss for iteration  522 is 0.45230827835909954\n",
      "loss for iteration  523 is 0.45230188706735885\n",
      "loss for iteration  524 is 0.4522955299076468\n",
      "loss for iteration  525 is 0.45228920644751175\n",
      "loss for iteration  526 is 0.45228291626083117\n",
      "loss for iteration  527 is 0.45227665892771096\n",
      "loss for iteration  528 is 0.4522704340343868\n",
      "loss for iteration  529 is 0.4522642411731263\n",
      "loss for iteration  530 is 0.4522580799421335\n",
      "loss for iteration  531 is 0.45225194994545487\n",
      "loss for iteration  532 is 0.45224585079288615\n",
      "loss for iteration  533 is 0.45223978209988236\n",
      "loss for iteration  534 is 0.4522337434874672\n",
      "loss for iteration  535 is 0.4522277345821458\n",
      "loss for iteration  536 is 0.452221755015818\n",
      "loss for iteration  537 is 0.4522158044256927\n",
      "loss for iteration  538 is 0.4522098824542053\n",
      "loss for iteration  539 is 0.4522039887489344\n",
      "loss for iteration  540 is 0.4521981229625213\n",
      "loss for iteration  541 is 0.4521922847525904\n",
      "loss for iteration  542 is 0.4521864737816708\n",
      "loss for iteration  543 is 0.4521806897171196\n",
      "loss for iteration  544 is 0.452174932231046\n",
      "loss for iteration  545 is 0.45216920100023683\n",
      "loss for iteration  546 is 0.4521634957060837\n",
      "loss for iteration  547 is 0.4521578160345106\n",
      "loss for iteration  548 is 0.45215216167590333\n",
      "loss for iteration  549 is 0.45214653232503976\n",
      "loss for iteration  550 is 0.45214092768102165\n",
      "loss for iteration  551 is 0.45213534744720696\n",
      "loss for iteration  552 is 0.45212979133114367\n",
      "loss for iteration  553 is 0.45212425904450504\n",
      "loss for iteration  554 is 0.45211875030302495\n",
      "loss for iteration  555 is 0.4521132648264354\n",
      "loss for iteration  556 is 0.4521078023384042\n",
      "loss for iteration  557 is 0.45210236256647424\n",
      "loss for iteration  558 is 0.45209694524200345\n",
      "loss for iteration  559 is 0.45209155010010593\n",
      "loss for iteration  560 is 0.4520861768795938\n",
      "loss for iteration  561 is 0.4520808253229204\n",
      "loss for iteration  562 is 0.4520754951761239\n",
      "loss for iteration  563 is 0.4520701861887722\n",
      "loss for iteration  564 is 0.45206489811390904\n",
      "loss for iteration  565 is 0.4520596307080001\n",
      "loss for iteration  566 is 0.4520543837308807\n",
      "loss for iteration  567 is 0.4520491569457041\n",
      "loss for iteration  568 is 0.45204395011889087\n",
      "loss for iteration  569 is 0.45203876302007867\n",
      "loss for iteration  570 is 0.45203359542207316\n",
      "loss for iteration  571 is 0.4520284471007998\n",
      "loss for iteration  572 is 0.45202331783525607\n",
      "loss for iteration  573 is 0.4520182074074646\n",
      "loss for iteration  574 is 0.45201311560242785\n",
      "loss for iteration  575 is 0.45200804220808166\n",
      "loss for iteration  576 is 0.4520029870152517\n",
      "loss for iteration  577 is 0.451997949817609\n",
      "loss for iteration  578 is 0.4519929304116274\n",
      "loss for iteration  579 is 0.4519879285965406\n",
      "loss for iteration  580 is 0.4519829441743009\n",
      "loss for iteration  581 is 0.451977976949538\n",
      "loss for iteration  582 is 0.4519730267295183\n",
      "loss for iteration  583 is 0.4519680933241056\n",
      "loss for iteration  584 is 0.4519631765457219\n",
      "loss for iteration  585 is 0.45195827620930884\n",
      "loss for iteration  586 is 0.45195339213228997\n",
      "loss for iteration  587 is 0.45194852413453357\n",
      "loss for iteration  588 is 0.45194367203831587\n",
      "loss for iteration  589 is 0.4519388356682853\n",
      "loss for iteration  590 is 0.4519340148514267\n",
      "loss for iteration  591 is 0.4519292094170265\n",
      "loss for iteration  592 is 0.4519244191966388\n",
      "loss for iteration  593 is 0.4519196440240511\n",
      "loss for iteration  594 is 0.45191488373525146\n",
      "loss for iteration  595 is 0.45191013816839554\n",
      "loss for iteration  596 is 0.45190540716377453\n",
      "loss for iteration  597 is 0.4519006905637837\n",
      "loss for iteration  598 is 0.4518959882128908\n",
      "loss for iteration  599 is 0.4518912999576058\n",
      "loss for iteration  600 is 0.4518866256464507\n",
      "loss for iteration  601 is 0.45188196512992984\n",
      "loss for iteration  602 is 0.4518773182605004\n",
      "loss for iteration  603 is 0.4518726848925442\n",
      "loss for iteration  604 is 0.45186806488233866\n",
      "loss for iteration  605 is 0.45186345808803013\n",
      "loss for iteration  606 is 0.45185886436960526\n",
      "loss for iteration  607 is 0.45185428358886476\n",
      "loss for iteration  608 is 0.45184971560939696\n",
      "loss for iteration  609 is 0.45184516029655114\n",
      "loss for iteration  610 is 0.4518406175174124\n",
      "loss for iteration  611 is 0.45183608714077605\n",
      "loss for iteration  612 is 0.451831569037123\n",
      "loss for iteration  613 is 0.45182706307859505\n",
      "loss for iteration  614 is 0.4518225691389707\n",
      "loss for iteration  615 is 0.4518180870936417\n",
      "loss for iteration  616 is 0.4518136168195897\n",
      "loss for iteration  617 is 0.45180915819536294\n",
      "loss for iteration  618 is 0.451804711101054\n",
      "loss for iteration  619 is 0.4518002754182774\n",
      "loss for iteration  620 is 0.4517958510301474\n",
      "loss for iteration  621 is 0.45179143782125714\n",
      "loss for iteration  622 is 0.4517870356776565\n",
      "loss for iteration  623 is 0.4517826444868321\n",
      "loss for iteration  624 is 0.45177826413768596\n",
      "loss for iteration  625 is 0.4517738945205158\n",
      "loss for iteration  626 is 0.45176953552699467\n",
      "loss for iteration  627 is 0.4517651870501518\n",
      "loss for iteration  628 is 0.45176084898435265\n",
      "loss for iteration  629 is 0.4517565212252806\n",
      "loss for iteration  630 is 0.4517522036699178\n",
      "loss for iteration  631 is 0.4517478962165266\n",
      "loss for iteration  632 is 0.4517435987646319\n",
      "loss for iteration  633 is 0.45173931121500294\n",
      "loss for iteration  634 is 0.45173503346963545\n",
      "loss for iteration  635 is 0.45173076543173507\n",
      "loss for iteration  636 is 0.45172650700569966\n",
      "loss for iteration  637 is 0.4517222580971027\n",
      "loss for iteration  638 is 0.4517180186126768\n",
      "loss for iteration  639 is 0.45171378846029747\n",
      "loss for iteration  640 is 0.4517095675489671\n",
      "loss for iteration  641 is 0.45170535578879867\n",
      "loss for iteration  642 is 0.4517011530910012\n",
      "loss for iteration  643 is 0.4516969593678633\n",
      "loss for iteration  644 is 0.4516927745327395\n",
      "loss for iteration  645 is 0.45168859850003373\n",
      "loss for iteration  646 is 0.4516844311851861\n",
      "loss for iteration  647 is 0.4516802725046576\n",
      "loss for iteration  648 is 0.45167612237591687\n",
      "loss for iteration  649 is 0.4516719807174252\n",
      "loss for iteration  650 is 0.4516678474486234\n",
      "loss for iteration  651 is 0.4516637224899181\n",
      "loss for iteration  652 is 0.4516596057626687\n",
      "loss for iteration  653 is 0.4516554971891737\n",
      "loss for iteration  654 is 0.4516513966926581\n",
      "loss for iteration  655 is 0.4516473041972607\n",
      "loss for iteration  656 is 0.45164321962802156\n",
      "loss for iteration  657 is 0.4516391429108693\n",
      "loss for iteration  658 is 0.45163507397260944\n",
      "loss for iteration  659 is 0.45163101274091205\n",
      "loss for iteration  660 is 0.45162695914429996\n",
      "loss for iteration  661 is 0.4516229131121374\n",
      "loss for iteration  662 is 0.4516188745746183\n",
      "loss for iteration  663 is 0.4516148434627549\n",
      "loss for iteration  664 is 0.45161081970836703\n",
      "loss for iteration  665 is 0.4516068032440706\n",
      "loss for iteration  666 is 0.45160279400326747\n",
      "loss for iteration  667 is 0.4515987919201342\n",
      "loss for iteration  668 is 0.4515947969296119\n",
      "loss for iteration  669 is 0.45159080896739606\n",
      "loss for iteration  670 is 0.45158682796992583\n",
      "loss for iteration  671 is 0.45158285387437463\n",
      "loss for iteration  672 is 0.45157888661863965\n",
      "loss for iteration  673 is 0.451574926141333\n",
      "loss for iteration  674 is 0.451570972381771\n",
      "loss for iteration  675 is 0.4515670252799659\n",
      "loss for iteration  676 is 0.4515630847766155\n",
      "loss for iteration  677 is 0.4515591508130948\n",
      "loss for iteration  678 is 0.4515552233314466\n",
      "loss for iteration  679 is 0.4515513022743726\n",
      "loss for iteration  680 is 0.4515473875852247\n",
      "loss for iteration  681 is 0.4515434792079964\n",
      "loss for iteration  682 is 0.4515395770873142\n",
      "loss for iteration  683 is 0.4515356811684291\n",
      "loss for iteration  684 is 0.45153179139720867\n",
      "loss for iteration  685 is 0.4515279077201284\n",
      "loss for iteration  686 is 0.45152403008426417\n",
      "loss for iteration  687 is 0.4515201584372838\n",
      "loss for iteration  688 is 0.45151629272744\n",
      "loss for iteration  689 is 0.45151243290356186\n",
      "loss for iteration  690 is 0.4515085789150476\n",
      "loss for iteration  691 is 0.4515047307118572\n",
      "loss for iteration  692 is 0.4515008882445049\n",
      "loss for iteration  693 is 0.45149705146405206\n",
      "loss for iteration  694 is 0.4514932203220998\n",
      "loss for iteration  695 is 0.45148939477078215\n",
      "loss for iteration  696 is 0.4514855747627588\n",
      "loss for iteration  697 is 0.4514817602512086\n",
      "loss for iteration  698 is 0.45147795118982254\n",
      "loss for iteration  699 is 0.45147414753279724\n",
      "loss for iteration  700 is 0.4514703492348282\n",
      "loss for iteration  701 is 0.4514665562511035\n",
      "loss for iteration  702 is 0.45146276853729733\n",
      "loss for iteration  703 is 0.4514589860495636\n",
      "loss for iteration  704 is 0.4514552087445299\n",
      "loss for iteration  705 is 0.45145143657929127\n",
      "loss for iteration  706 is 0.45144766951140414\n",
      "loss for iteration  707 is 0.4514439074988806\n",
      "loss for iteration  708 is 0.4514401505001819\n",
      "loss for iteration  709 is 0.45143639847421363\n",
      "loss for iteration  710 is 0.45143265138031924\n",
      "loss for iteration  711 is 0.45142890917827455\n",
      "loss for iteration  712 is 0.4514251718282825\n",
      "loss for iteration  713 is 0.45142143929096745\n",
      "loss for iteration  714 is 0.4514177115273696\n",
      "loss for iteration  715 is 0.45141398849894004\n",
      "loss for iteration  716 is 0.4514102701675353\n",
      "loss for iteration  717 is 0.4514065564954123\n",
      "loss for iteration  718 is 0.4514028474452227\n",
      "loss for iteration  719 is 0.4513991429800088\n",
      "loss for iteration  720 is 0.45139544306319784\n",
      "loss for iteration  721 is 0.45139174765859735\n",
      "loss for iteration  722 is 0.4513880567303901\n",
      "loss for iteration  723 is 0.45138437024312994\n",
      "loss for iteration  724 is 0.45138068816173615\n",
      "loss for iteration  725 is 0.4513770104514896\n",
      "loss for iteration  726 is 0.45137333707802796\n",
      "loss for iteration  727 is 0.4513696680073407\n",
      "loss for iteration  728 is 0.4513660032057655\n",
      "loss for iteration  729 is 0.4513623426399831\n",
      "loss for iteration  730 is 0.4513586862770133\n",
      "loss for iteration  731 is 0.45135503408421085\n",
      "loss for iteration  732 is 0.4513513860292606\n",
      "loss for iteration  733 is 0.451347742080174\n",
      "loss for iteration  734 is 0.4513441022052847\n",
      "loss for iteration  735 is 0.4513404663732445\n",
      "loss for iteration  736 is 0.45133683455301943\n",
      "loss for iteration  737 is 0.4513332067138856\n",
      "loss for iteration  738 is 0.4513295828254257\n",
      "loss for iteration  739 is 0.45132596285752447\n",
      "loss for iteration  740 is 0.4513223467803658\n",
      "loss for iteration  741 is 0.4513187345644282\n",
      "loss for iteration  742 is 0.4513151261804817\n",
      "loss for iteration  743 is 0.45131152159958365\n",
      "loss for iteration  744 is 0.4513079207930756\n",
      "loss for iteration  745 is 0.4513043237325794\n",
      "loss for iteration  746 is 0.45130073038999396\n",
      "loss for iteration  747 is 0.45129714073749166\n",
      "loss for iteration  748 is 0.4512935547475149\n",
      "loss for iteration  749 is 0.4512899723927729\n",
      "loss for iteration  750 is 0.4512863936462381\n",
      "loss for iteration  751 is 0.4512828184811431\n",
      "loss for iteration  752 is 0.4512792468709773\n",
      "loss for iteration  753 is 0.451275678789484\n",
      "loss for iteration  754 is 0.45127211421065666\n",
      "loss for iteration  755 is 0.4512685531087362\n",
      "loss for iteration  756 is 0.45126499545820803\n",
      "loss for iteration  757 is 0.45126144123379863\n",
      "loss for iteration  758 is 0.4512578904104728\n",
      "loss for iteration  759 is 0.4512543429634308\n",
      "loss for iteration  760 is 0.451250798868105\n",
      "loss for iteration  761 is 0.4512472581001574\n",
      "loss for iteration  762 is 0.4512437206354767\n",
      "loss for iteration  763 is 0.45124018645017533\n",
      "loss for iteration  764 is 0.45123665552058695\n",
      "loss for iteration  765 is 0.4512331278232633\n",
      "loss for iteration  766 is 0.451229603334972\n",
      "loss for iteration  767 is 0.45122608203269327\n",
      "loss for iteration  768 is 0.45122256389361787\n",
      "loss for iteration  769 is 0.4512190488951444\n",
      "loss for iteration  770 is 0.45121553701487627\n",
      "loss for iteration  771 is 0.45121202823061946\n",
      "loss for iteration  772 is 0.4512085225203803\n",
      "loss for iteration  773 is 0.4512050198623626\n",
      "loss for iteration  774 is 0.4512015202349651\n",
      "loss for iteration  775 is 0.4511980236167795\n",
      "loss for iteration  776 is 0.45119452998658793\n",
      "loss for iteration  777 is 0.45119103932336013\n",
      "loss for iteration  778 is 0.4511875516062518\n",
      "loss for iteration  779 is 0.45118406681460194\n",
      "loss for iteration  780 is 0.4511805849279305\n",
      "loss for iteration  781 is 0.45117710592593646\n",
      "loss for iteration  782 is 0.45117362978849535\n",
      "loss for iteration  783 is 0.451170156495657\n",
      "loss for iteration  784 is 0.45116668602764376\n",
      "loss for iteration  785 is 0.45116321836484785\n",
      "loss for iteration  786 is 0.45115975348782983\n",
      "loss for iteration  787 is 0.45115629137731594\n",
      "loss for iteration  788 is 0.4511528320141964\n",
      "loss for iteration  789 is 0.4511493753795234\n",
      "loss for iteration  790 is 0.45114592145450866\n",
      "loss for iteration  791 is 0.45114247022052206\n",
      "loss for iteration  792 is 0.4511390216590893\n",
      "loss for iteration  793 is 0.45113557575189006\n",
      "loss for iteration  794 is 0.45113213248075607\n",
      "loss for iteration  795 is 0.451128691827669\n",
      "loss for iteration  796 is 0.4511252537747591\n",
      "loss for iteration  797 is 0.45112181830430303\n",
      "loss for iteration  798 is 0.45111838539872207\n",
      "loss for iteration  799 is 0.4511149550405801\n",
      "loss for iteration  800 is 0.45111152721258213\n",
      "loss for iteration  801 is 0.45110810189757267\n",
      "loss for iteration  802 is 0.45110467907853363\n",
      "loss for iteration  803 is 0.4511012587385827\n",
      "loss for iteration  804 is 0.45109784086097166\n",
      "loss for iteration  805 is 0.45109442542908496\n",
      "loss for iteration  806 is 0.45109101242643757\n",
      "loss for iteration  807 is 0.4510876018366738\n",
      "loss for iteration  808 is 0.4510841936435655\n",
      "loss for iteration  809 is 0.45108078783101024\n",
      "loss for iteration  810 is 0.4510773843830302\n",
      "loss for iteration  811 is 0.45107398328377013\n",
      "loss for iteration  812 is 0.45107058451749626\n",
      "loss for iteration  813 is 0.45106718806859447\n",
      "loss for iteration  814 is 0.4510637939215687\n",
      "loss for iteration  815 is 0.4510604020610397\n",
      "loss for iteration  816 is 0.4510570124717435\n",
      "loss for iteration  817 is 0.4510536251385299\n",
      "loss for iteration  818 is 0.4510502400463611\n",
      "loss for iteration  819 is 0.45104685718031007\n",
      "loss for iteration  820 is 0.45104347652555954\n",
      "loss for iteration  821 is 0.4510400980674001\n",
      "loss for iteration  822 is 0.4510367217912292\n",
      "loss for iteration  823 is 0.45103334768254977\n",
      "loss for iteration  824 is 0.45102997572696857\n",
      "loss for iteration  825 is 0.45102660591019544\n",
      "loss for iteration  826 is 0.4510232382180413\n",
      "loss for iteration  827 is 0.45101987263641724\n",
      "loss for iteration  828 is 0.4510165091513331\n",
      "loss for iteration  829 is 0.45101314774889667\n",
      "loss for iteration  830 is 0.4510097884153116\n",
      "loss for iteration  831 is 0.45100643113687666\n",
      "loss for iteration  832 is 0.45100307589998456\n",
      "loss for iteration  833 is 0.4509997226911207\n",
      "loss for iteration  834 is 0.4509963714968618\n",
      "loss for iteration  835 is 0.45099302230387467\n",
      "loss for iteration  836 is 0.45098967509891547\n",
      "loss for iteration  837 is 0.45098632986882803\n",
      "loss for iteration  838 is 0.4509829866005429\n",
      "loss for iteration  839 is 0.45097964528107665\n",
      "loss for iteration  840 is 0.4509763058975297\n",
      "loss for iteration  841 is 0.4509729684370863\n",
      "loss for iteration  842 is 0.45096963288701297\n",
      "loss for iteration  843 is 0.45096629923465736\n",
      "loss for iteration  844 is 0.45096296746744713\n",
      "loss for iteration  845 is 0.4509596375728892\n",
      "loss for iteration  846 is 0.4509563095385684\n",
      "loss for iteration  847 is 0.4509529833521466\n",
      "loss for iteration  848 is 0.45094965900136147\n",
      "loss for iteration  849 is 0.45094633647402593\n",
      "loss for iteration  850 is 0.4509430157580266\n",
      "loss for iteration  851 is 0.45093969684132307\n",
      "loss for iteration  852 is 0.45093637971194694\n",
      "loss for iteration  853 is 0.4509330643580007\n",
      "loss for iteration  854 is 0.450929750767657\n",
      "loss for iteration  855 is 0.4509264389291575\n",
      "loss for iteration  856 is 0.4509231288308119\n",
      "loss for iteration  857 is 0.4509198204609975\n",
      "loss for iteration  858 is 0.45091651380815717\n",
      "loss for iteration  859 is 0.4509132088607999\n",
      "loss for iteration  860 is 0.45090990560749866\n",
      "loss for iteration  861 is 0.4509066040368902\n",
      "loss for iteration  862 is 0.4509033041376738\n",
      "loss for iteration  863 is 0.450900005898611\n",
      "loss for iteration  864 is 0.4508967093085237\n",
      "loss for iteration  865 is 0.45089341435629415\n",
      "loss for iteration  866 is 0.45089012103086423\n",
      "loss for iteration  867 is 0.45088682932123364\n",
      "loss for iteration  868 is 0.45088353921646007\n",
      "loss for iteration  869 is 0.4508802507056578\n",
      "loss for iteration  870 is 0.45087696377799735\n",
      "loss for iteration  871 is 0.4508736784227039\n",
      "loss for iteration  872 is 0.4508703946290576\n",
      "loss for iteration  873 is 0.45086711238639177\n",
      "loss for iteration  874 is 0.4508638316840928\n",
      "loss for iteration  875 is 0.4508605525115988\n",
      "loss for iteration  876 is 0.4508572748583995\n",
      "loss for iteration  877 is 0.450853998714035\n",
      "loss for iteration  878 is 0.4508507240680953\n",
      "loss for iteration  879 is 0.4508474509102191\n",
      "loss for iteration  880 is 0.45084417923009396\n",
      "loss for iteration  881 is 0.4508409090174545\n",
      "loss for iteration  882 is 0.4508376402620826\n",
      "loss for iteration  883 is 0.4508343729538062\n",
      "loss for iteration  884 is 0.4508311070824986\n",
      "loss for iteration  885 is 0.450827842638078\n",
      "loss for iteration  886 is 0.4508245796105067\n",
      "loss for iteration  887 is 0.45082131798979025\n",
      "loss for iteration  888 is 0.4508180577659771\n",
      "loss for iteration  889 is 0.45081479892915777\n",
      "loss for iteration  890 is 0.4508115414694641\n",
      "loss for iteration  891 is 0.45080828537706885\n",
      "loss for iteration  892 is 0.45080503064218475\n",
      "loss for iteration  893 is 0.4508017772550639\n",
      "loss for iteration  894 is 0.45079852520599756\n",
      "loss for iteration  895 is 0.450795274485315\n",
      "loss for iteration  896 is 0.4507920250833831\n",
      "loss for iteration  897 is 0.4507887769906059\n",
      "loss for iteration  898 is 0.4507855301974234\n",
      "loss for iteration  899 is 0.45078228469431186\n",
      "loss for iteration  900 is 0.45077904047178247\n",
      "loss for iteration  901 is 0.45077579752038094\n",
      "loss for iteration  902 is 0.4507725558306872\n",
      "loss for iteration  903 is 0.4507693153933144\n",
      "loss for iteration  904 is 0.4507660761989087\n",
      "loss for iteration  905 is 0.45076283823814833\n",
      "loss for iteration  906 is 0.4507596015017435\n",
      "loss for iteration  907 is 0.45075636598043545\n",
      "loss for iteration  908 is 0.450753131664996\n",
      "loss for iteration  909 is 0.4507498985462271\n",
      "loss for iteration  910 is 0.4507466666149602\n",
      "loss for iteration  911 is 0.4507434358620559\n",
      "loss for iteration  912 is 0.4507402062784029\n",
      "loss for iteration  913 is 0.4507369778549182\n",
      "loss for iteration  914 is 0.450733750582546\n",
      "loss for iteration  915 is 0.4507305244522575\n",
      "loss for iteration  916 is 0.45072729945505025\n",
      "loss for iteration  917 is 0.4507240755819477\n",
      "loss for iteration  918 is 0.45072085282399876\n",
      "loss for iteration  919 is 0.45071763117227714\n",
      "loss for iteration  920 is 0.4507144106178809\n",
      "loss for iteration  921 is 0.4507111911519322\n",
      "loss for iteration  922 is 0.45070797276557634\n",
      "loss for iteration  923 is 0.4507047554499819\n",
      "loss for iteration  924 is 0.4507015391963397\n",
      "loss for iteration  925 is 0.45069832399586285\n",
      "loss for iteration  926 is 0.45069510983978567\n",
      "loss for iteration  927 is 0.4506918967193636\n",
      "loss for iteration  928 is 0.45068868462587314\n",
      "loss for iteration  929 is 0.45068547355061045\n",
      "loss for iteration  930 is 0.4506822634848917\n",
      "loss for iteration  931 is 0.45067905442005235\n",
      "loss for iteration  932 is 0.4506758463474466\n",
      "loss for iteration  933 is 0.45067263925844714\n",
      "loss for iteration  934 is 0.45066943314444474\n",
      "loss for iteration  935 is 0.4506662279968476\n",
      "loss for iteration  936 is 0.4506630238070809\n",
      "loss for iteration  937 is 0.45065982056658693\n",
      "loss for iteration  938 is 0.450656618266824\n",
      "loss for iteration  939 is 0.4506534168992663\n",
      "loss for iteration  940 is 0.4506502164554038\n",
      "loss for iteration  941 is 0.45064701692674136\n",
      "loss for iteration  942 is 0.45064381830479827\n",
      "loss for iteration  943 is 0.45064062058110843\n",
      "loss for iteration  944 is 0.4506374237472196\n",
      "loss for iteration  945 is 0.45063422779469303\n",
      "loss for iteration  946 is 0.4506310327151028\n",
      "loss for iteration  947 is 0.45062783850003607\n",
      "loss for iteration  948 is 0.4506246451410922\n",
      "loss for iteration  949 is 0.45062145262988257\n",
      "loss for iteration  950 is 0.4506182609580302\n",
      "loss for iteration  951 is 0.4506150701171691\n",
      "loss for iteration  952 is 0.45061188009894443\n",
      "loss for iteration  953 is 0.45060869089501165\n",
      "loss for iteration  954 is 0.45060550249703657\n",
      "loss for iteration  955 is 0.45060231489669456\n",
      "loss for iteration  956 is 0.45059912808567043\n",
      "loss for iteration  957 is 0.4505959420556582\n",
      "loss for iteration  958 is 0.4505927567983608\n",
      "loss for iteration  959 is 0.45058957230548896\n",
      "loss for iteration  960 is 0.45058638856876204\n",
      "loss for iteration  961 is 0.4505832055799068\n",
      "loss for iteration  962 is 0.4505800233306572\n",
      "loss for iteration  963 is 0.45057684181275487\n",
      "loss for iteration  964 is 0.4505736610179475\n",
      "loss for iteration  965 is 0.45057048093798946\n",
      "loss for iteration  966 is 0.4505673015646411\n",
      "loss for iteration  967 is 0.45056412288966846\n",
      "loss for iteration  968 is 0.4505609449048431\n",
      "loss for iteration  969 is 0.45055776760194155\n",
      "loss for iteration  970 is 0.45055459097274525\n",
      "loss for iteration  971 is 0.45055141500903995\n",
      "loss for iteration  972 is 0.4505482397026157\n",
      "loss for iteration  973 is 0.4505450650452663\n",
      "loss for iteration  974 is 0.4505418910287892\n",
      "loss for iteration  975 is 0.45053871764498504\n",
      "loss for iteration  976 is 0.4505355448856574\n",
      "loss for iteration  977 is 0.45053237274261243\n",
      "loss for iteration  978 is 0.45052920120765894\n",
      "loss for iteration  979 is 0.45052603027260735\n",
      "loss for iteration  980 is 0.4505228599292703\n",
      "loss for iteration  981 is 0.4505196901694617\n",
      "loss for iteration  982 is 0.45051652098499656\n",
      "loss for iteration  983 is 0.4505133523676911\n",
      "loss for iteration  984 is 0.450510184309362\n",
      "loss for iteration  985 is 0.4505070168018263\n",
      "loss for iteration  986 is 0.4505038498369013\n",
      "loss for iteration  987 is 0.4505006834064039\n",
      "loss for iteration  988 is 0.45049751750215056\n",
      "loss for iteration  989 is 0.45049435211595734\n",
      "loss for iteration  990 is 0.450491187239639\n",
      "loss for iteration  991 is 0.45048802286500905\n",
      "loss for iteration  992 is 0.45048485898387963\n",
      "loss for iteration  993 is 0.45048169558806106\n",
      "loss for iteration  994 is 0.45047853266936166\n",
      "loss for iteration  995 is 0.45047537021958733\n",
      "loss for iteration  996 is 0.4504722082305416\n",
      "loss for iteration  997 is 0.4504690466940252\n",
      "loss for iteration  998 is 0.4504658856018356\n",
      "loss for iteration  999 is 0.4504627249457673\n",
      "loss for iteration  1000 is 0.45045956471761084\n",
      "test loss is  0.4540638890633703\n",
      "Test Accuracy: 0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))\n",
    "def sigmoidder(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "x = data\n",
    "x = sc.fit_transform(x)\n",
    "w1= np.random.rand(6,8)*0.01\n",
    "b1 = np.zeros((8,))\n",
    "w2 = np.random.rand(1,8)*0.01\n",
    "b2 = np.zeros((1,))\n",
    "alpha = 0.4\n",
    "for i in range (1000):\n",
    "    z1 = np.dot(x,w1) + b1;    \n",
    "    a1 = sigmoid(z1);   \n",
    "\n",
    "\n",
    "    z2 = np.dot(a1,w2.T)+b2\n",
    "    a2 = sigmoid(z2)\n",
    "    dz2 = a2 - y\n",
    "    # print(\"shape dz2\",dz2.shape)\n",
    "    # print(\"shape a1\",a1.shape)\n",
    "    dw2 = (1/m)*np.dot(dz2.T,a1)\n",
    "    db2 = (1/m)*np.sum(dz2,axis=0)\n",
    "    dz1 = np.dot(dz2,w2)*sigmoidder(z1)\n",
    "    dw1 = (1/m)*np.dot(x.T,dz1)\n",
    "    db1 = (1/m)*np.sum(dz1,axis=0)\n",
    "    w2 = w2 - alpha*dw2\n",
    "    b2 = b2 - alpha*db2\n",
    "    w1 = w1 - alpha*dw1\n",
    "    b1 = b1 - alpha*db1 \n",
    "    J = -(1/m)*np.sum((y*np.log(a2)+(1-y)*np.log(1-a2)))\n",
    "\n",
    "    print(\"loss for iteration \",i+1,\"is\",J)\n",
    "\n",
    "df1 = pd.read_csv('test.csv')\n",
    "Xt = df1.drop([\"Survived\",\"Name\",'Ticket','Cabin','Embarked','PassengerId',], axis=1)        \n",
    "m1 = Xt.shape[0]\n",
    "Yt = df1['Survived'].values.reshape(-1,1)\n",
    "Xt['Sex'] = Xt['Sex'].map({'male': 1 ,'female':2})\n",
    "Xt['Age'] = Xt['Age'].fillna(Xt['Age'].mean())\n",
    "\n",
    "Xt = sc.transform(Xt)\n",
    "z1 = np.dot(Xt,w1) + b1;    \n",
    "a1 = sigmoid(z1);   \n",
    "z2 = np.dot(a1,w2.T)+b2\n",
    "a2 = sigmoid(z2)\n",
    "a2 = np.clip(a2, 1e-15, 1-1e-15)\n",
    "Jt = -(1/m1)*np.sum((Yt*np.log(a2)+(1-Yt)*np.log(1-a2)))\n",
    "print(\"test loss is \",Jt)\n",
    "y_pred = (a2 >= 0.5).astype(int)\n",
    "accuracy = np.mean(y_pred == Yt)    \n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49647e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
